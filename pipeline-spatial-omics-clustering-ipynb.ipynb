{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10113440,"sourceType":"datasetVersion","datasetId":6094392}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nimport networkx as nx\nfrom scipy.spatial.distance import cdist\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nimport seaborn as sns\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster import hierarchy\nimport time\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:12:37.030970Z","iopub.execute_input":"2024-12-06T14:12:37.031344Z","iopub.status.idle":"2024-12-06T14:12:37.059218Z","shell.execute_reply.started":"2024-12-06T14:12:37.031313Z","shell.execute_reply":"2024-12-06T14:12:37.058137Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dlpfc-151669/151672_count_matrix.csv\n/kaggle/input/dlpfc-151669/151674_meta_data.csv\n/kaggle/input/dlpfc-151669/151509_meta_data.csv\n/kaggle/input/dlpfc-151669/151508_meta_data.csv\n/kaggle/input/dlpfc-151669/151671_meta_data.csv\n/kaggle/input/dlpfc-151669/151670_count_matrix.csv\n/kaggle/input/dlpfc-151669/151509_count_matrix.csv\n/kaggle/input/dlpfc-151669/151507_count_matrix.csv\n/kaggle/input/dlpfc-151669/151507_meta_data.csv\n/kaggle/input/dlpfc-151669/151672_meta_data.csv\n/kaggle/input/dlpfc-151669/151671_count_matrix.csv\n/kaggle/input/dlpfc-151669/151670_meta_data.csv\n/kaggle/input/dlpfc-151669/151669_count_matrix.csv\n/kaggle/input/dlpfc-151669/151675_meta_data.csv\n/kaggle/input/dlpfc-151669/151674_count_matrix.csv\n/kaggle/input/dlpfc-151669/151508_count_matrix.csv\n/kaggle/input/dlpfc-151669/151675_count_matrix.csv\n/kaggle/input/dlpfc-151669/151673_count_matrix.csv\n/kaggle/input/dlpfc-151669/151676_meta_data.csv\n/kaggle/input/dlpfc-151669/151510_meta_data.csv\n/kaggle/input/dlpfc-151669/151673_meta_data.csv\n/kaggle/input/dlpfc-151669/151676_count_matrix.csv\n/kaggle/input/dlpfc-151669/151669_meta_data.csv\n/kaggle/input/dlpfc-151669/151510_count_matrix.csv\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Load metadata and count matrix from 12 samples","metadata":{}},{"cell_type":"code","source":"samples = {}\nsample_numbers = ['151507', '151508', '151509', '151510', '151669', '151670', '151671', '151672', '151673', '151674', '151675', '151676']\n\nfor i in sample_numbers:\n    m_file = f\"/kaggle/input/dlpfc-151669/{i}_meta_data.csv\"  # Adjust for each sample's metadata file\n    c_file = f\"/kaggle/input/dlpfc-151669/{i}_count_matrix.csv\"  # Adjust for each sample's count matrix file\n    \n    # Read the CSV files for metadata and count matrix\n    m = pd.read_csv(m_file, index_col=0)\n    c = pd.read_csv(c_file, index_col=0)\n\n    # Remove the rows with NaN from both 'meta' and 'counts' DataFrames\n    nan_index = m[m['Layer'].isna()].index\n    m = m.drop(nan_index)\n    c = c.drop(nan_index)\n    \n    # Convert ground truth cluster assignment to categorical\n    m['Layer'] = pd.Categorical(m['Layer'])\n\n    # Scale for PCA\n    scaler = StandardScaler()\n    c_scaled = scaler.fit_transform(c)\n    \n    # Store both the metadata and count matrix as a list for each sample\n    samples[f'Sample {i}'] = [m, c_scaled, c]\n\n# m1 = pd.read_csv(\"/kaggle/input/dlpfc-151669/151669_meta_data.csv\", index_col =0) \n# c1 = pd.read_csv(\"/kaggle/input/dlpfc-151669/151669_count_matrix.csv\", index_col =0)\n# samples['Sample 1'] = [m1, c1]\n\n# m2 = pd.read_csv(\"/kaggle/input/dlpfc-151669/151669_meta_data.csv\", index_col =0) \n# c2 = pd.read_csv(\"/kaggle/input/dlpfc-151669/151669_count_matrix.csv\", index_col =0)\n# samples['Sample 1'] = [m2, c2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:12:37.061439Z","iopub.execute_input":"2024-12-06T14:12:37.061918Z","iopub.status.idle":"2024-12-06T14:12:42.546198Z","shell.execute_reply.started":"2024-12-06T14:12:37.061860Z","shell.execute_reply":"2024-12-06T14:12:42.545359Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## FUNCTION: Loop through 12 datasets and perform PCA, aggregation, and clustering with desired parameters. Save clustering plot and metrics.","metadata":{}},{"cell_type":"code","source":"# counts = samples['sample_1'][0] # Extract count matrix\n# meta = samples['sample_1'][1] # Extract metadata\n\ndef run_samples(PCA_amount, num_hops, weight_weights, param_string):\n    # Empty dictionary for clustering performance metrics\n    metrics = {}\n    # Iterate over each sample in dictionary\n    for sample in samples:\n        count_scaled = samples[sample][1] # Extract scaled count matrix\n        meta = samples[sample][0] # Extract metadata\n        count = samples[sample][2] # Extract count matrix\n        file_name = f'({param_string}) {sample}'\n    \n        # PCA\n        pca = PCA(n_components = PCA_amount)\n        count_pca_array = pca.fit_transform(count_scaled)\n        count_pca = pd.DataFrame(count_pca_array,index=count.index)\n    \n        # Create graph object\n        G = create_graph(sample, file_name, count_pca, meta)\n        \n        # Aggregate neighborhood features and concatenate with own features\n        node_features = torch.from_numpy(count_pca_array)\n        aggregated_features = aggregate_k_hops(G, node_features, num_hops, weight_weights)\n        own_and_aggregated_features = torch.cat((node_features, aggregated_features), dim=1)\n\n        # Determine number of clusters using inertia elbow method, cluster using KMeans++\n        k_values = range(5, 15)\n        k , labels = best_k(k_values, own_and_aggregated_features, meta=None, figFile=f\"{file_name}: Elbow Plot.png\")\n        meta['Clusters'] = labels\n    \n        # Compute metrics and save to dict\n        ari = adjusted_rand_score(meta['Layer'], labels)\n        metrics[sample] = [labels, ari, k]\n    \n        # Generate ground truth and clustering result spatial plots (colored by cluster)\n        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n        \n        sns.scatterplot(x='X', y='Y', hue='Layer', data=meta, palette='Set1', s=10, ax=axs[0])\n        axs[0].set_title(f\"{sample}: Ground Truth\")    \n        \n        sns.scatterplot(x='X', y='Y', hue='Clusters', data=meta, palette='Set1', s=10,ax=axs[1])\n        axs[1].set_title(f\"{sample}: Clusters\")\n        \n        plt.tight_layout() # Adjust spacing between subplots\n        plt.savefig(f'/kaggle/working/{file_name}: Cluster Plot.png')\n        # plt.show()\n        plt.close()\n\n    # Compute average and sd of metrics (ARI)\n    metrics_matrix = np.array([value[1:] for value in metrics.values()])\n    avg_ari = np.mean(metrics_matrix[:, 0])\n    sd_ari = np.std(metrics_matrix[:, 0])\n    \n    return metrics, avg_ari, sd_ari","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:12:42.561926Z","iopub.execute_input":"2024-12-06T14:12:42.562248Z","iopub.status.idle":"2024-12-06T14:12:42.581555Z","shell.execute_reply.started":"2024-12-06T14:12:42.562218Z","shell.execute_reply":"2024-12-06T14:12:42.580464Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## FUNCTION: Create graph object from spatial coordinates and save plot","metadata":{}},{"cell_type":"code","source":"def create_graph(sample, file_name, count_pca, meta):\n    num_spots = np.shape(count_pca)[0]  # number of spots (nodes)\n    spatial_coordinates = meta.iloc[:,0:2]\n \n    # Create graph objet\n    G = nx.Graph()\n     \n    # Add nodes with their gene expression PC data as features\n    for i in range(num_spots):\n        G.add_node(i, features=torch.tensor(count_pca.iloc[i,], dtype=torch.float32))\n     \n    # Compute pairwise Euclidean distances between spots based on their spatial coordinates\n    distance_matrix = cdist(spatial_coordinates, spatial_coordinates)\n     \n    # Define a distance threshold for creating edges based on spatial proximity\n    distance_threshold = 150\n     \n    # Add edges based on the spatial distance threshold\n    for i in range(num_spots):\n        for j in range(i + 1, num_spots):\n            if distance_matrix[i, j] < distance_threshold:\n                G.add_edge(i, j)\n    \n    # Save node/edge plot    \n    positions = {i: spatial_coordinates.iloc[i] for i in range(num_spots)}\n    plt.figure(figsize=(10, 10)) \n    nx.draw(G, pos=positions, with_labels=False, node_size=10, node_color=\"red\", font_size=12, font_weight=\"bold\")\n    plt.title(f\"{sample}: Node and Edge Plot\", fontsize=16)\n     \n    plt.savefig(f'/kaggle/working/{file_name}: Node and Edge Plot.png')\n    # plt.show()\n    plt.close()\n\n    return G","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:12:42.582942Z","iopub.execute_input":"2024-12-06T14:12:42.583376Z","iopub.status.idle":"2024-12-06T14:12:42.601258Z","shell.execute_reply.started":"2024-12-06T14:12:42.583333Z","shell.execute_reply":"2024-12-06T14:12:42.600184Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## FUNCTION: Aggregate neighborhood information from graph","metadata":{}},{"cell_type":"code","source":"def aggregate_k_hops(graph, node_features, k, weight_weights):\n    weights = generate_exponential_decay_values(k) * weight_weights\n    feature_dim = node_features.shape[1]\n    aggregated_features = torch.zeros(node_features.shape[0], feature_dim * 1)\n    \n    for node in graph.nodes():\n        current_neighbors = {node}\n        all_neighbors = set()\n        weighted_mean_sum = torch.zeros(feature_dim)\n        feature_parts = []\n        \n        for hop in range(1, k + 1):\n            # Expand to the next hop\n            next_neighbors = set()\n            for n in current_neighbors:\n                next_neighbors.update(graph.neighbors(n))\n            current_neighbors = next_neighbors - all_neighbors - {node}  # Avoid duplicates and self-loops\n            all_neighbors.update(current_neighbors)\n            \n            # Aggregate features for the current hop\n            if len(current_neighbors) > 0:\n                neighbor_features = node_features[list(current_neighbors)]\n                mean_features = neighbor_features.mean(dim=0) \n                # max_features = neighbor_features.max(dim=0).values\n            else:\n                mean_features = torch.zeros(feature_dim) \n                # max_features = torch.zeros(feature_dim)\n            \n            # feature_parts.extend([mean_features, max_features])\n            # feature_parts.extend([mean_features])\n            weighted_mean_sum += weights[hop - 1] * mean_features\n        \n        # Concatenate all aggregated features for this node\n        # aggregated_features[node] = torch.cat(feature_parts)\n        # aggregated_features[node] = torch.cat([weighted_mean_sum, max_features])\n        aggregated_features[node] = weighted_mean_sum\n    \n    return aggregated_features\n\n\ndef generate_exponential_decay_values(k, rate=.5):\n    # Generate k values that decay exponentially: e^(-rate * i)\n    indices = np.arange(1, k + 1)  # Generate indices from 1 to k\n    exp_values = np.exp(-rate * indices)  # Apply exponential decay\n    \n    # Normalize the values so that their sum equals 1\n    normalized_values = exp_values / np.sum(exp_values)\n    \n    return normalized_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:12:42.602749Z","iopub.execute_input":"2024-12-06T14:12:42.603633Z","iopub.status.idle":"2024-12-06T14:12:42.620056Z","shell.execute_reply.started":"2024-12-06T14:12:42.603591Z","shell.execute_reply":"2024-12-06T14:12:42.619138Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## FUNCTION: Clustering","metadata":{}},{"cell_type":"code","source":"def find_elbow_point(k_values, costs):\n    # Normalize k_values and costs to [0, 1] for comparison\n    k_values_normalized = (k_values - np.min(k_values)) / (np.max(k_values) - np.min(k_values))\n    costs_normalized = (costs - np.min(costs)) / (np.max(costs) - np.min(costs))\n    # Calculate the difference from the line connecting first and last points\n    line = np.array([k_values_normalized, costs_normalized]).T\n    start, end = line[0], line[-1]\n    distances = np.abs(np.cross(end-start, line-start) / np.linalg.norm(end-start))\n    # The index with the maximum distance is the elbow point\n    elbow_idx = np.argmax(distances)\n    return k_values[elbow_idx], elbow_idx\n\ndef best_k(k_values, own_and_aggregated_features, meta=None, figFile=\"output.png\"):\n    inertia_values = []\n    labels = []\n \n    # Loop through the range of k values\n    for i in k_values:\n        kmeans = KMeans(n_clusters=i, init='k-means++', algorithm=\"lloyd\", n_init='auto', random_state=0)\n        kmeans.fit(own_and_aggregated_features)\n        agg_labels = kmeans.labels_\n        inertia_values.append(kmeans.inertia_)\n        labels.append(agg_labels)\n        if meta is not None:\n            ari = adjusted_rand_score(meta['Layer'], agg_labels)\n            print(f\"k = {i}, Adjusted Rand Index (ARI): {ari}, inertia: {kmeans.inertia_}\")\n    # Plot the Elbow Method\n    plt.figure(figsize=(10, 5))\n    plt.plot(k_values, inertia_values, 'o-', label=\"Inertia (Elbow Method)\")\n    plt.xlabel(\"Number of Clusters (k)\")\n    plt.ylabel(\"Inertia\")\n    plt.title(\"Elbow Method\")\n    plt.legend()\n    plt.grid()\n    plt.savefig(figFile)\n    plt.close()\n    # Determine the optimal k using the Elbow Method\n    best_k, best_idx = find_elbow_point(k_values, inertia_values)\n    # print(f\"The optimal k (Elbow Point) is: {best_k}\")\n    # Return the best k and the corresponding cluster labels\n    return best_k, labels[best_idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:12:42.621307Z","iopub.execute_input":"2024-12-06T14:12:42.621696Z","iopub.status.idle":"2024-12-06T14:12:42.636990Z","shell.execute_reply.started":"2024-12-06T14:12:42.621642Z","shell.execute_reply":"2024-12-06T14:12:42.636087Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## Perform hyperparameter search","metadata":{}},{"cell_type":"code","source":"# Hyperparameter search values\nhyp_weight_weights = [1.25, 1.5]\nhyp_num_hops = [1, 2, 3, 4, 5]\nhyp_PCA = [99, 90, 80]\n\n# hyp_PCA = [80]\n# hyp_num_hops = [1, 2]\n# hyp_weight_weights = [1]\n\noutput = {}\nt = 0\nfor weight_weights in hyp_weight_weights:\n    for num_hops in hyp_num_hops:\n        for PCA_amount in hyp_PCA:\n            t += 1\n            param_string = f'PCA:{PCA_amount}, Num Hops:{num_hops}, Neighborhood Mult:{weight_weights}'\n\n            # Run samples with specified hyperparameters\n            start_time = time.time()\n            metrics, avg_ari, sd_ari = run_samples(PCA_amount, num_hops, weight_weights, param_string)\n            end_time = time.time()\n            execution_time = end_time - start_time\n\n            # Save and print\n            output[t] = [PCA_amount, num_hops, weight_weights, metrics, avg_ari, sd_ari]\n            print(t, execution_time, avg_ari, sd_ari, PCA_amount, num_hops, weight_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:12:42.638254Z","iopub.execute_input":"2024-12-06T14:12:42.638534Z","iopub.status.idle":"2024-12-06T14:12:51.040907Z","shell.execute_reply.started":"2024-12-06T14:12:42.638509Z","shell.execute_reply":"2024-12-06T14:12:51.039794Z"}},"outputs":[{"name":"stdout","text":"1 4.3250391483306885 0.016540315877545276 0.00484682207561269 80 1 1\n2 4.061120510101318 0.011975379450292395 0.013492690770180695 80 2 1\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import pickle\n\n# Save the dictionary to a pickle file\nwith open('output_dict.pkl', 'wb') as file:\n    pickle.dump(output, file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:12:51.042221Z","iopub.execute_input":"2024-12-06T14:12:51.042613Z","iopub.status.idle":"2024-12-06T14:12:51.048682Z","shell.execute_reply.started":"2024-12-06T14:12:51.042572Z","shell.execute_reply":"2024-12-06T14:12:51.047647Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# metrics, avg_ari, sd_ari = run_samples(PCA_amount=80, num_hops=3, weight_weights=1.25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:12:51.051161Z","iopub.execute_input":"2024-12-06T14:12:51.051473Z","iopub.status.idle":"2024-12-06T14:12:51.062089Z","shell.execute_reply.started":"2024-12-06T14:12:51.051431Z","shell.execute_reply":"2024-12-06T14:12:51.061176Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# print(avg_ari, sd_ari)\n# print(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:12:51.063262Z","iopub.execute_input":"2024-12-06T14:12:51.063587Z","iopub.status.idle":"2024-12-06T14:12:51.075490Z","shell.execute_reply.started":"2024-12-06T14:12:51.063556Z","shell.execute_reply":"2024-12-06T14:12:51.074531Z"}},"outputs":[],"execution_count":29}]}